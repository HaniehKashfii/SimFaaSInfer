# LLaMA2-70B Model Configuration

model:
  name: "llama2-70b"
  num_layers: 80
  hidden_size: 8192
  intermediate_size: 28672
  num_attention_heads: 64
  num_kv_heads: 8  # GQA
  vocab_size: 32000
  max_position_embeddings: 4096

  # Architectural details
  architecture: "llama"
  attention_type: "gqa"  # Grouped Query Attention
  activation_function: "silu"
  rope_theta: 10000

  # Computed parameters
  head_dim: 128
  kv_channels: 1024  # num_kv_heads * head_dim

profiling:
  # Token-level operators (ms per token)
  attention_prefill_time_per_token_sq: 0.000025
  attention_decode_time_per_token: 0.0003
  mlp_time_per_token: 0.0008
  layernorm_time_per_token: 0.00005
  embedding_time_per_token: 0.00008

  # Communication operators (ms per GB)
  allreduce_time_per_gb: 5.0
  allgather_time_per_gb: 4.5
  send_recv_time_per_gb: 3.0

  # Memory footprint (GB)
  model_memory_gb: 140.0
  kv_cache_memory_per_token_mb: 0.0004  # per layer (GQA uses less)

cluster:
  num_replicas: 2
  gpu_type: "A100"
  num_gpus_per_replica: 4

parallelization:
  tensor_parallel_size: 4
  pipeline_parallel_size: 1

scheduler:
  type: "sarathi"
  max_batch_size: 256
  max_tokens_per_batch: 2048
  chunk_size: 512  # for Sarathi