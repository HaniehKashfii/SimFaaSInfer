run_type,seq_len,batch_size,op_name,cuda_time_total_ms,cuda_time_mean_ms,cpu_time_total_ms,cuda_occurrences,input_shapes,extra_tags
prefill,512,8,inference_forward,0.0,0.0,17.294810000000002,1,,
prefill,512,8,aten::view,0.0,0.0,0.3127739999999874,201,,
prefill,512,8,aten::embedding,0.0,0.0,2.6245819999999997,2,,
prefill,512,8,aten::reshape,0.0,0.0,0.04824799999999811,15,,
prefill,512,8,aten::index_select,0.0,0.0,2.601402,2,,
prefill,512,8,aten::empty,0.0,0.0,0.6021970000000046,198,,
prefill,512,8,aten::resize_,0.0,0.0,0.014458999999999946,3,,
prefill,512,8,Activity Buffer Request,0.0,0.0,2.49875,1,,
prefill,512,8,cudaLaunchKernel,0.0,0.0,1.8970180000000105,234,,
prefill,512,8,inference_forward,0.0,0.0,0.0,1,,
prefill,512,8,"void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<long const, unsigned int>, int, int, unsigned int, unsigned int, long)",0.0,0.0,0.0,2,,
prefill,512,8,aten::arange,0.0,0.0,0.0590619999999999,2,,
prefill,512,8,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>::result_type*)",0.0,0.0,0.0,1,,
prefill,512,8,aten::unsqueeze,0.0,0.0,0.008519999999999982,1,,
prefill,512,8,aten::as_strided,0.0,0.0,0.12100000000001683,137,,
prefill,512,8,aten::to,0.0,0.0,0.27932800000000363,37,,
prefill,512,8,aten::add,0.0,0.0,0.8094050000000002,49,,
prefill,512,8,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0.0,0.0,0.0,1,,
prefill,512,8,aten::dropout,0.0,0.0,0.010170999999996183,25,,
prefill,512,8,aten::layer_norm,0.0,0.0,0.9779220000000018,25,,
prefill,512,8,aten::native_layer_norm,0.0,0.0,0.8812509999999966,25,,
prefill,512,8,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0.0,0.0,0.0,25,,
prefill,512,8,aten::addmm,0.0,0.0,1.9063050000000012,48,,
prefill,512,8,ampere_sgemm_128x64_nn,0.0,0.0,0.0,24,,
prefill,512,8,aten::split,0.0,0.0,0.2578030000000008,12,,
prefill,512,8,aten::narrow,0.0,0.0,0.17404299999998965,36,,
prefill,512,8,aten::slice,0.0,0.0,0.14104100000000472,39,,
prefill,512,8,aten::transpose,0.0,0.0,0.25605899999998927,97,,
prefill,512,8,aten::_to_copy,0.0,0.0,0.2398050000000003,24,,
prefill,512,8,aten::empty_strided,0.0,0.0,0.1016630000000032,24,,
prefill,512,8,aten::copy_,0.0,0.0,0.040472999999997226,24,,
prefill,512,8,aten::lift_fresh,0.0,0.0,0.0025820000000021538,24,,
prefill,512,8,aten::detach_,0.0,0.0,0.027961999999999536,24,,
prefill,512,8,detach_,0.0,0.0,0.007879000000000814,24,,
prefill,512,8,aten::cat,0.0,0.0,0.5398000000000011,24,,
prefill,512,8,aten::scaled_dot_product_attention,0.0,0.0,0.7206420000000017,12,,
prefill,512,8,aten::_scaled_dot_product_efficient_attention,0.0,0.0,0.6076130000000031,12,,
prefill,512,8,aten::_efficient_attention_forward,0.0,0.0,0.37934799999999996,12,,
prefill,512,8,cudaStreamIsCapturing,0.0,0.0,0.013327999999997701,12,,
prefill,512,8,cudaOccupancyMaxActiveBlocksPerMultiprocessor,0.0,0.0,0.023830000000003564,13,,
prefill,512,8,aten::mul,0.0,0.0,0.6707509999999965,48,,
prefill,512,8,aten::pow,0.0,0.0,0.37032999999999994,12,,
prefill,512,8,aten::result_type,0.0,0.0,0.003518999999997504,12,,
prefill,512,8,"void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0.0,0.0,0.0,24,,
prefill,512,8,aten::tanh,0.0,0.0,0.13746299999999792,12,,
prefill,512,8,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0.0,0.0,0.0,12,,
prefill,512,8,ampere_sgemm_128x32_nn,0.0,0.0,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",0.0,0.0,0.0,36,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",0.0,0.0,0.0,36,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#2}, std::array<char*, 2ul> >(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#2}, std::array<char*, 2ul>)",0.0,0.0,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>)",0.0,0.0,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul>)",0.0,0.0,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",0.0,0.0,0.0,12,,
prefill,512,8,ampere_sgemm_64x32_sliced1x4_nn,0.0,0.0,0.0,12,,
prefill,512,8,aten::linear,0.0,0.0,0.08275200000000041,1,,
prefill,512,8,aten::t,0.0,0.0,0.005860000000000582,1,,
prefill,512,8,aten::matmul,0.0,0.0,0.0666820000000007,1,,
prefill,512,8,aten::mm,0.0,0.0,0.048861000000000786,1,,
prefill,512,8,aten::_unsafe_view,0.0,0.0,0.00575,1,,
prefill,512,8,cudaDeviceSynchronize,0.0,0.0,55.855577999999994,1,,
prefill,512,8,ampere_sgemm_128x64_tn,0.0,0.0,0.0,1,,
prefill,512,8,inference_forward,0.0,0.0,17.258969,1,,
prefill,512,8,aten::view,0.0,0.0,0.31475000000001374,201,,
prefill,512,8,aten::embedding,0.0,0.0,2.5258709999999995,2,,
prefill,512,8,aten::reshape,0.0,0.0,0.05159199999999714,15,,
prefill,512,8,aten::index_select,0.0,0.0,2.5020010000000004,2,,
prefill,512,8,aten::empty,0.0,0.0,0.5960079999999769,198,,
prefill,512,8,aten::resize_,0.0,0.0,0.014769999999999868,3,,
prefill,512,8,Activity Buffer Request,0.0,0.0,2.4015280000000003,1,,
prefill,512,8,cudaLaunchKernel,0.0,0.0,1.9142199999999916,234,,
prefill,512,8,inference_forward,0.0,0.0,0.0,1,,
prefill,512,8,"void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<long const, unsigned int>, int, int, unsigned int, unsigned int, long)",0.0,0.0,0.0,2,,
prefill,512,8,aten::arange,0.0,0.0,0.055170999999999824,2,,
prefill,512,8,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>::result_type*)",0.0,0.0,0.0,1,,
prefill,512,8,aten::unsqueeze,0.0,0.0,0.008130999999999859,1,,
prefill,512,8,aten::as_strided,0.0,0.0,0.1107979999999934,137,,
prefill,512,8,aten::to,0.0,0.0,0.24169500000000152,37,,
prefill,512,8,aten::add,0.0,0.0,0.8221620000000085,49,,
prefill,512,8,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0.0,0.0,0.0,1,,
prefill,512,8,aten::dropout,0.0,0.0,0.010791999999999916,25,,
prefill,512,8,aten::layer_norm,0.0,0.0,1.0193829999999953,25,,
prefill,512,8,aten::native_layer_norm,0.0,0.0,0.9189780000000014,25,,
prefill,512,8,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0.0,0.0,0.0,25,,
prefill,512,8,aten::addmm,0.0,0.0,1.901614999999999,48,,
prefill,512,8,ampere_sgemm_128x64_nn,0.0,0.0,0.0,24,,
prefill,512,8,aten::split,0.0,0.0,0.2702990000000068,12,,
prefill,512,8,aten::narrow,0.0,0.0,0.19106500000000506,36,,
prefill,512,8,aten::slice,0.0,0.0,0.1434970000000044,39,,
prefill,512,8,aten::transpose,0.0,0.0,0.2700690000000036,97,,
prefill,512,8,aten::_to_copy,0.0,0.0,0.20931400000000122,24,,
prefill,512,8,aten::empty_strided,0.0,0.0,0.09239499999999953,24,,
prefill,512,8,aten::copy_,0.0,0.0,0.04048800000000574,24,,
prefill,512,8,aten::lift_fresh,0.0,0.0,0.0026470000000017534,24,,
prefill,512,8,aten::detach_,0.0,0.0,0.026960000000003675,24,,
prefill,512,8,detach_,0.0,0.0,0.006719000000002779,24,,
prefill,512,8,aten::cat,0.0,0.0,0.5502180000000008,24,,
prefill,512,8,aten::scaled_dot_product_attention,0.0,0.0,0.7374169999999958,12,,
prefill,512,8,aten::_scaled_dot_product_efficient_attention,0.0,0.0,0.6274020000000009,12,,
prefill,512,8,aten::_efficient_attention_forward,0.0,0.0,0.3981089999999995,12,,
prefill,512,8,cudaStreamIsCapturing,0.0,0.0,0.013149999999998727,12,,
prefill,512,8,cudaOccupancyMaxActiveBlocksPerMultiprocessor,0.0,0.0,0.022923000000000682,13,,
prefill,512,8,aten::mul,0.0,0.0,0.6988509999999979,48,,
prefill,512,8,aten::pow,0.0,0.0,0.4208289999999988,12,,
prefill,512,8,aten::result_type,0.0,0.0,0.003369000000001506,12,,
prefill,512,8,"void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0.0,0.0,0.0,24,,
prefill,512,8,aten::tanh,0.0,0.0,0.14651400000000286,12,,
prefill,512,8,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0.0,0.0,0.0,12,,
prefill,512,8,ampere_sgemm_128x32_nn,0.0,0.0,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",0.0,0.0,0.0,36,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",0.0,0.0,0.0,36,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#2}, std::array<char*, 2ul> >(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#2}, std::array<char*, 2ul>)",0.0,0.0,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>)",0.0,0.0,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul>)",0.0,0.0,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",0.0,0.0,0.0,12,,
prefill,512,8,ampere_sgemm_64x32_sliced1x4_nn,0.0,0.0,0.0,12,,
prefill,512,8,aten::linear,0.0,0.0,0.07651100000000224,1,,
prefill,512,8,aten::t,0.0,0.0,0.006099999999998545,1,,
prefill,512,8,aten::matmul,0.0,0.0,0.06077100000000064,1,,
prefill,512,8,aten::mm,0.0,0.0,0.04766100000000006,1,,
prefill,512,8,aten::_unsafe_view,0.0,0.0,0.0017900000000008731,1,,
prefill,512,8,cudaDeviceSynchronize,0.0,0.0,55.617913,1,,
prefill,512,8,ampere_sgemm_128x64_tn,0.0,0.0,0.0,1,,
prefill,512,8,inference_forward,69.39163699999995,69.39163699999995,17.595736,1,,
prefill,512,8,aten::view,0.0,0.0,0.3063519999999993,201,,
prefill,512,8,aten::embedding,0.09686199999999963,0.048430999999999814,2.674503,2,,
prefill,512,8,aten::reshape,0.0,0.0,0.04583000000000265,15,,
prefill,512,8,aten::index_select,0.09686199999999963,0.048430999999999814,2.6515229999999996,2,,
prefill,512,8,aten::empty,0.0,0.0,0.6479569999999963,198,,
prefill,512,8,aten::resize_,0.0,0.0,0.013630000000000224,3,,
prefill,512,8,Activity Buffer Request,0.043998999999999795,0.043998999999999795,2.5534010000000005,1,,
prefill,512,8,cudaLaunchKernel,0.0,0.0,1.9214269999999816,234,,
prefill,512,8,inference_forward,70.182837,70.182837,0.0,1,,
prefill,512,8,"void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<long const, unsigned int>, int, int, unsigned int, unsigned int, long)",0.05286299999999983,0.026431499999999913,0.0,2,,
prefill,512,8,aten::arange,0.0038400000000001454,0.0019200000000000727,0.05645100000000048,2,,
prefill,512,8,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>::result_type*)",0.0019200000000000727,0.0019200000000000727,0.0,1,,
prefill,512,8,aten::unsqueeze,0.0,0.0,0.0084699999999998,1,,
prefill,512,8,aten::as_strided,0.0,0.0,0.10769700000000011,137,,
prefill,512,8,aten::to,0.0,0.0,0.2786269999999963,37,,
prefill,512,8,aten::add,2.8515609999999665,0.058195122448978905,0.8258139999999948,49,,
prefill,512,8,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0.017248000000000048,0.017248000000000048,0.0,1,,
prefill,512,8,aten::dropout,0.0,0.0,0.010491000000000895,25,,
prefill,512,8,aten::layer_norm,0.5111630000000146,0.020446520000000582,1.0559220000000051,25,,
prefill,512,8,aten::native_layer_norm,0.5111630000000146,0.020446520000000582,0.9528609999999912,25,,
prefill,512,8,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0.5111630000000146,0.020446520000000582,0.0,25,,
prefill,512,8,aten::addmm,39.81874,0.8295570833333332,1.9573910000000074,48,,
prefill,512,8,ampere_sgemm_128x64_nn,22.523447999999988,0.9384769999999996,0.0,24,,
prefill,512,8,aten::split,0.0,0.0,0.2645560000000005,12,,
prefill,512,8,aten::narrow,0.0,0.0,0.17769500000000243,36,,
prefill,512,8,aten::slice,0.0,0.0,0.13001099999999952,39,,
prefill,512,8,aten::transpose,0.0,0.0,0.30166699999999946,97,,
prefill,512,8,aten::_to_copy,0.0,0.0,0.24633600000000844,24,,
prefill,512,8,aten::empty_strided,0.0,0.0,0.11007999999999811,24,,
prefill,512,8,aten::copy_,0.0,0.0,0.040888999999995575,24,,
prefill,512,8,aten::lift_fresh,0.0,0.0,0.0025520000000005894,24,,
prefill,512,8,aten::detach_,0.0,0.0,0.027762999999994462,24,,
prefill,512,8,detach_,0.0,0.0,0.007273000000002867,24,,
prefill,512,8,aten::cat,0.8247640000000083,0.034365166666667016,0.5611499999999996,24,,
prefill,512,8,aten::scaled_dot_product_attention,2.7967449999999983,0.23306208333333317,0.7300960000000023,12,,
prefill,512,8,aten::_scaled_dot_product_efficient_attention,2.7967449999999983,0.23306208333333317,0.6177230000000018,12,,
prefill,512,8,aten::_efficient_attention_forward,2.7967449999999983,0.23306208333333317,0.37638800000000266,12,,
prefill,512,8,cudaStreamIsCapturing,0.0,0.0,0.012630000000003748,12,,
prefill,512,8,cudaOccupancyMaxActiveBlocksPerMultiprocessor,0.0,0.0,0.02376100000000042,13,,
prefill,512,8,aten::mul,4.031524999999964,0.08399010416666591,0.6662779999999994,48,,
prefill,512,8,aten::pow,0.8929199999999937,0.07440999999999948,0.5416019999999989,12,,
prefill,512,8,aten::result_type,0.0,0.0,0.003630999999998494,12,,
prefill,512,8,"void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0.8247640000000083,0.034365166666667016,0.0,24,,
prefill,512,8,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",2.7967449999999983,0.23306208333333317,0.0,12,,
prefill,512,8,aten::tanh,0.9327610000000041,0.07773008333333367,0.13867300000000704,12,,
prefill,512,8,ampere_sgemm_128x32_nn,3.634019000000002,0.30283491666666684,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",1.9108949999999623,0.05308041666666562,0.0,36,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",2.643407999999977,0.07342799999999935,0.0,36,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#2}, std::array<char*, 2ul> >(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#2}, std::array<char*, 2ul>)",0.8929199999999937,0.07440999999999948,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>)",0.9327610000000041,0.07773008333333367,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul>)",0.9234180000000042,0.07695150000000035,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",1.3881169999999874,0.11567641666666562,0.0,12,,
prefill,512,8,ampere_sgemm_64x32_sliced1x4_nn,13.661273000000008,1.1384394166666674,0.0,12,,
prefill,512,8,aten::linear,16.632675999999993,16.632675999999993,0.07805199999999968,1,,
prefill,512,8,aten::t,0.0,0.0,0.005899999999997818,1,,
prefill,512,8,aten::matmul,16.632675999999993,16.632675999999993,0.06188100000000122,1,,
prefill,512,8,aten::mm,16.632675999999993,16.632675999999993,0.04802100000000064,1,,
prefill,512,8,aten::_unsafe_view,0.0,0.0,0.0019500000000007276,1,,
prefill,512,8,cudaDeviceSynchronize,0.0,0.0,55.457989999999995,1,,
prefill,512,8,ampere_sgemm_128x64_tn,16.632675999999993,16.632675999999993,0.0,1,,
prefill,512,8,inference_forward,69.39931799999998,69.39931799999998,17.634756000000003,1,,
prefill,512,8,aten::view,0.0,0.0,0.3026880000000042,201,,
prefill,512,8,aten::embedding,0.10198400000000038,0.05099200000000019,2.7659160000000003,2,,
prefill,512,8,aten::reshape,0.0,0.0,0.04791100000000097,15,,
prefill,512,8,aten::index_select,0.10198400000000038,0.05099200000000019,2.743716,2,,
prefill,512,8,aten::empty,0.0,0.0,0.5901309999999891,198,,
prefill,512,8,aten::resize_,0.0,0.0,0.012369000000000484,3,,
prefill,512,8,Activity Buffer Request,0.046720000000000254,0.046720000000000254,2.655414,1,,
prefill,512,8,cudaLaunchKernel,0.0,0.0,1.9309380000000083,234,,
prefill,512,8,inference_forward,70.136796,70.136796,0.0,1,,
prefill,512,8,"void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<long const, unsigned int>, int, int, unsigned int, unsigned int, long)",0.055264000000000126,0.027632000000000063,0.0,2,,
prefill,512,8,aten::arange,0.0039039999999995415,0.0019519999999997708,0.04946000000000049,2,,
prefill,512,8,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>::result_type*)",0.0019519999999997708,0.0019519999999997708,0.0,1,,
prefill,512,8,aten::unsqueeze,0.0,0.0,0.006159999999999855,1,,
prefill,512,8,aten::as_strided,0.0,0.0,0.14090399999999317,137,,
prefill,512,8,aten::to,0.0,0.0,0.2624850000000033,37,,
prefill,512,8,aten::add,2.839520999999984,0.05794940816326498,0.8230320000000042,49,,
prefill,512,8,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0.01750400000000036,0.01750400000000036,0.0,1,,
prefill,512,8,aten::dropout,0.0,0.0,0.010931000000001405,25,,
prefill,512,8,aten::layer_norm,0.5097280000000042,0.020389120000000167,0.9683179999999979,25,,
prefill,512,8,aten::native_layer_norm,0.5097280000000042,0.020389120000000167,0.8683139999999976,25,,
prefill,512,8,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0.5097280000000042,0.020389120000000167,0.0,25,,
prefill,512,8,aten::addmm,39.82392999999999,0.8296652083333332,1.9488269999999899,48,,
prefill,512,8,ampere_sgemm_128x64_nn,22.522579,0.9384407916666667,0.0,24,,
prefill,512,8,aten::split,0.0,0.0,0.26757199999999737,12,,
prefill,512,8,aten::narrow,0.0,0.0,0.18587999999999374,36,,
prefill,512,8,aten::slice,0.0,0.0,0.14068300000000453,39,,
prefill,512,8,aten::transpose,0.0,0.0,0.26151800000000547,97,,
prefill,512,8,aten::_to_copy,0.0,0.0,0.23145599999999922,24,,
prefill,512,8,aten::empty_strided,0.0,0.0,0.09741699999999673,24,,
prefill,512,8,aten::copy_,0.0,0.0,0.04041000000000531,24,,
prefill,512,8,aten::lift_fresh,0.0,0.0,0.002430999999998676,24,,
prefill,512,8,aten::detach_,0.0,0.0,0.030171000000001186,24,,
prefill,512,8,detach_,0.0,0.0,0.007688999999996667,24,,
prefill,512,8,aten::cat,0.8257950000000092,0.03440812500000038,0.5345789999999961,24,,
prefill,512,8,aten::scaled_dot_product_attention,2.8147259999999967,0.23456049999999973,0.7253949999999968,12,,
prefill,512,8,aten::_scaled_dot_product_efficient_attention,2.8147259999999967,0.23456049999999973,0.6164339999999966,12,,
prefill,512,8,aten::_efficient_attention_forward,2.8147259999999967,0.23456049999999973,0.38022699999999715,12,,
prefill,512,8,cudaStreamIsCapturing,0.0,0.0,0.014819000000002233,14,,
prefill,512,8,cudaOccupancyMaxActiveBlocksPerMultiprocessor,0.0,0.0,0.02333099999999922,13,,
prefill,512,8,aten::mul,4.030951999999982,0.0839781666666663,0.8717099999999964,48,,
prefill,512,8,aten::pow,0.8910110000000087,0.07425091666666737,0.3248230000000003,12,,
prefill,512,8,aten::result_type,0.0,0.0,0.0038500000000040017,12,,
prefill,512,8,"void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0.8257950000000092,0.03440812500000038,0.0,24,,
prefill,512,8,aten::tanh,0.9250580000000027,0.07708816666666689,0.13564199999999801,12,,
prefill,512,8,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",2.8147259999999967,0.23456049999999973,0.0,12,,
prefill,512,8,ampere_sgemm_128x32_nn,3.6395599999999986,0.30329666666666655,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",1.905920999999993,0.0529422499999998,0.0,36,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",2.6429169999999895,0.07341436111111083,0.0,36,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#2}, std::array<char*, 2ul> >(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#2}, std::array<char*, 2ul>)",0.8910110000000087,0.07425091666666737,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>)",0.9250580000000027,0.07708816666666689,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul>)",0.9160959999999905,0.07634133333333254,0.0,12,,
prefill,512,8,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",1.3880349999999926,0.11566958333333272,0.0,12,,
prefill,512,8,ampere_sgemm_64x32_sliced1x4_nn,13.661790999999997,1.1384825833333332,0.0,12,,
prefill,512,8,cudaMalloc,0.0,0.0,0.360836000000003,2,,
prefill,512,8,aten::linear,16.634661,16.634661,0.3132459999999992,1,,
prefill,512,8,aten::t,0.0,0.0,0.005649999999997817,1,,
prefill,512,8,aten::matmul,16.634661,16.634661,0.2957960000000021,1,,
prefill,512,8,aten::mm,16.634661,16.634661,0.2808559999999998,1,,
prefill,512,8,aten::_unsafe_view,0.0,0.0,0.002,1,,
prefill,512,8,cudaDeviceSynchronize,0.0,0.0,55.441659,1,,
prefill,512,8,ampere_sgemm_128x64_tn,16.634661,16.634661,0.0,1,,
